#!/usr/bin/env python3
"""
Markdown export for bug finder results.
Generates concise, Slack-friendly summaries.
"""

import json
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime


def export_to_markdown(
    matches: List[Dict[str, Any]],
    output_file: Path,
    metadata: Dict[str, Any],
    include_context: bool = True,
    max_urls: int = None
) -> None:
    """
    Export bug scan results to Markdown format for Slack/chat.

    Args:
        matches: List of match dictionaries
        output_file: Path to output markdown file
        metadata: Scan metadata
        include_context: Whether to include scan context at top
        max_urls: Maximum number of URLs to include (default: None = all URLs)
    """
    total_bugs = len(matches)
    scan_date = metadata.get('scan_date', datetime.now().strftime('%Y-%m-%d'))
    site_scanned = metadata.get('site_scanned', 'Unknown')
    pages_scanned = metadata.get('pages_scanned', 'Unknown')

    # Build markdown content
    lines = []

    if include_context:
        lines.append(f"# Bug Scan Results: {site_scanned}")
        lines.append("")
        lines.append(f"**Scan Date:** {scan_date}")
        lines.append(f"**Pages Scanned:** {pages_scanned:,}" if isinstance(pages_scanned, int) else f"**Pages Scanned:** {pages_scanned}")
        lines.append(f"**Issues Found:** {total_bugs} pages with similar bugs")
        lines.append("")
        lines.append("---")
        lines.append("")

    # Add affected URLs
    urls_to_show = max_urls if max_urls is not None else total_bugs

    if max_urls is not None and total_bugs > max_urls:
        lines.append(f"## Affected Pages ({urls_to_show} of {total_bugs} shown)")
    else:
        lines.append(f"## Affected Pages ({total_bugs} total)")
    lines.append("")

    for i, match in enumerate(matches[:urls_to_show], 1):
        url = match['url']
        lines.append(f"{i}. {url}")

    # Add truncation notice if needed
    if max_urls is not None and total_bugs > max_urls:
        lines.append("")
        lines.append(f"*...and {total_bugs - max_urls} more pages*")

    # Add footer
    lines.append("")
    lines.append("---")
    lines.append("")
    lines.append("*Generated by website-analyzer bug finder*")

    # Write to file
    markdown_content = "\n".join(lines)
    output_file.write_text(markdown_content, encoding='utf-8')


def export_to_markdown_grouped(
    matches: List[Dict[str, Any]],
    output_file: Path,
    metadata: Dict[str, Any],
    include_context: bool = True
) -> None:
    """
    Export bug scan results grouped by priority/severity.

    Args:
        matches: List of match dictionaries
        output_file: Path to output markdown file
        metadata: Scan metadata
        include_context: Whether to include scan context at top
    """
    total_bugs = len(matches)
    scan_date = metadata.get('scan_date', datetime.now().strftime('%Y-%m-%d'))
    site_scanned = metadata.get('site_scanned', 'Unknown')
    pages_scanned = metadata.get('pages_scanned', 'Unknown')

    # Group by priority
    by_priority = {'critical': [], 'high': [], 'medium': [], 'low': []}
    for match in matches:
        priority = match.get('priority', 'medium')
        if priority in by_priority:
            by_priority[priority].append(match)

    # Build markdown content
    lines = []

    if include_context:
        lines.append(f"# Bug Scan Results: {site_scanned}")
        lines.append("")
        lines.append(f"**Scan Date:** {scan_date}")
        lines.append(f"**Pages Scanned:** {pages_scanned:,}" if isinstance(pages_scanned, int) else f"**Pages Scanned:** {pages_scanned}")
        lines.append(f"**Issues Found:** {total_bugs} pages")
        lines.append("")

        # Priority breakdown
        lines.append("**By Priority:**")
        for priority in ['critical', 'high', 'medium', 'low']:
            count = len(by_priority[priority])
            if count > 0:
                emoji = {'critical': 'ðŸ”´', 'high': 'ðŸŸ ', 'medium': 'ðŸŸ¡', 'low': 'âšª'}[priority]
                lines.append(f"- {emoji} {priority.title()}: {count}")
        lines.append("")
        lines.append("---")
        lines.append("")

    # Add URLs by priority
    for priority in ['critical', 'high', 'medium', 'low']:
        if not by_priority[priority]:
            continue

        emoji = {'critical': 'ðŸ”´', 'high': 'ðŸŸ ', 'medium': 'ðŸŸ¡', 'low': 'âšª'}[priority]
        lines.append(f"## {emoji} {priority.title()} Priority ({len(by_priority[priority])} pages)")
        lines.append("")

        for match in by_priority[priority]:
            url = match['url']
            lines.append(f"- {url}")

        lines.append("")

    # Add footer
    lines.append("---")
    lines.append("")
    lines.append("*Generated by website-analyzer bug finder*")

    # Write to file
    markdown_content = "\n".join(lines)
    output_file.write_text(markdown_content, encoding='utf-8')


def export_to_slack_snippet(
    matches: List[Dict[str, Any]],
    output_file: Path,
    metadata: Dict[str, Any],
    max_urls: int = 20
) -> None:
    """
    Export ultra-concise version for pasting directly into Slack.
    Minimal formatting, maximum copy-paste friendliness.

    Args:
        matches: List of match dictionaries
        output_file: Path to output text file
        metadata: Scan metadata
        max_urls: Maximum URLs to include (default: 20 for quick scan)
    """
    total_bugs = len(matches)
    site_scanned = metadata.get('site_scanned', 'Unknown')

    lines = []

    # One-line summary
    lines.append(f"Found {total_bugs} pages on {site_scanned} with similar bugs:")
    lines.append("")

    # Just the URLs
    for match in matches[:max_urls]:
        lines.append(match['url'])

    # Truncation notice
    if total_bugs > max_urls:
        lines.append("")
        lines.append(f"...and {total_bugs - max_urls} more")

    # Write to file
    content = "\n".join(lines)
    output_file.write_text(content, encoding='utf-8')


if __name__ == '__main__':
    import sys

    if len(sys.argv) < 2:
        print("Usage: python bug_finder_export_markdown.py <input_json> [output_file] [--slack]")
        print("  --slack: Generate ultra-concise Slack snippet (default: standard markdown)")
        sys.exit(1)

    input_file = sys.argv[1]
    use_slack = '--slack' in sys.argv

    # Determine output filename
    if len(sys.argv) >= 3 and not sys.argv[2].startswith('--'):
        output_file = Path(sys.argv[2])
    else:
        input_path = Path(input_file)
        suffix = '_slack.txt' if use_slack else '.md'
        output_file = input_path.with_suffix(suffix)

    # Load results
    with open(input_file, 'r') as f:
        data = json.load(f)

    matches = data.get('results', [])
    metadata = data.get('metadata', {})

    # Export
    if use_slack:
        export_to_slack_snippet(matches, output_file, metadata)
        print(f"âœ… Slack snippet saved to: {output_file}")
    else:
        export_to_markdown(matches, output_file, metadata)
        print(f"âœ… Markdown report saved to: {output_file}")

    print(f"   {len(matches)} pages with bugs")
    print(f"\nðŸ“‹ Ready to copy-paste into Slack/chat!")
